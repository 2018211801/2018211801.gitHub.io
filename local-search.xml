<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/"/>
    <url>/blog/</url>
    
    <content type="html"><![CDATA[<h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>Hi, I am Xiaochen Wang, a master student at PKU-NLP group, supervised by Prof. Zhifang Sui. Previously, I completed my bachelor’s degree at Beijing University of Posts and Telecommunications (BUPT) in July 2022.</p><p><strong>I am looking for a phd opportunity, please contact me if you are interested.</strong></p><p>I am broadly interested in natural language processing and machine learning.  My current research focuses on enhancing the reasoning capabilities of LLMs and efficient training.<br> I am also interested in multimodal and embodied intelligence because they can be implemented in real-world applications in the future.  </p><p>I am happy to discuss potential collaboration opportunities, feel free to reach out!</p><h2 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h2><p>PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization  2023.12 - 2024.03<br> {Problems:} Rank adjustment during training is fixed and optimal rank value is uncertain. Besides, parameter scale for LoRA conflicts with memory usage.</p><p> {Solutions:} We propose PeriodicLoRA (PLoRA), a method that periodically unloads the LoRA matrices back to the original backbone with a ratio. This process generates a higher rank update matrix by accumulating low rank matrices without extra memory usage.</p><p>{Results:} Experimental findings demonstrate that PLoRA with low rank outperforms LoRA, improving training speed by 80%.   </p><p>LoRA++, Adding an Implicit Momentum SGD  Optimizer for LoRAs, in progress{2024.05 - present}}<br>{Problems：}<br>  When tuning PLoRA, we discovered that the parameter update process can be viewed from the perspective of an optimizer, with the update interval treated as the batch size and PLoRA resembling momentum-SGD.<br>{Solutions：}<br> Inspiring from optimization, we incorporated dynamic learning rates and batch size to relieve overfitting. Besides, we imitate the AdamW to make the parameter updates smoother.<br>{Results：}<br> Our method, employing a lower-rank parameter scale, can outperform higher-rank LoRA and perform on par with full fine-tuning.</p><p>RouterLoRA, LoRA Meets MOE, exploring{2024.03 - 2024.04}<br>{Problems：}<br>  Using LoRA combined with MOE resolves the capability conflicts in multitask training by incorporating multiple LoRA modules as experts to isolate the parameter space. There are two different projects.</p><p>{Solutions:}<br>  Instead of topk strategy, we fix foundational expert to share common knowledge or information avoiding knowledge redundancy in different experts.<br>{Solutions:}<br>Pre-trained LLMs is equipped with foundational knowledge and do not require finetuning for certain data. We design a dynamic selection (0-k) of experts, allowing the model to learn how to choose a number of experts for processing.</p><p> FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering{2023.12 - 2024.02}}<br>{Problems:}<br>  Large Language models (LLMs) with few-shot methods underperform in multi-hop reasoning because of hallucination, error propagation and limited context length.<br>{Solutions:}<br>  We design a zero-shot FSM framework based on Finite State Machine that explicitly supports the model in the phases of decomposition, retrieval, and verification. FSM tackles one task at a time, decides the next action based on the current results and states.<br>{Results:}<br>  Our results have a more standardized format and outperform COT on challenging datasets like Musique.</p><p>Can Large Multimodal Models Uncover Deep Semantics Behind Comic Strip?, in progress{2024.07 - present}<br>{Problems：}Comic strips can be considered as important frames in video. We use them to evaluate the capability of video-LLM to understand the deep meanings of sequential multiple images.<br>{Solutions：}<br> We design three tasks: next frame prediction, Reorganize and deep meaning to evaluate.</p><p>Statistical Dataset Evaluation: Reliability, Difficulty, and Validity{2022.09 - 2023.01}}<br>{Problems:}<br>  Existing datasets have exposed numerous issues, leading to biased models and unreliable evaluation, without human evaluation of dataset quality.<br>f{Solutions:}<br>  We seek to understand the statistical properties of datasets and address three fundamental dimensions: reliability, difficulty, and validity. Taking the Named Entity Recognition datasets as a case study, we introduce $9$ statistical metrics.</p><p>{Results:}<br> We studied how the scores of datasets on statistical metrics impact model performance and advocate for assessing dataset quality or making targeted improvements to datasets before training or testing models.</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
